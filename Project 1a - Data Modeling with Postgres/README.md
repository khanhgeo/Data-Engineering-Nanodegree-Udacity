
### Project description
Sparkify company is been collecting data on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

In this project, I would like to create a Postgres database with tables designed to optimize queries on song play analysis. In order to achieve the goal, a database schema and ETL pipeline will be emplemented for this analysis. In order to help Sparkify achieve their analytics goal, the database is designed to has a fact table named songplay and four dim tables, namely users, songs, artists and time. This designed database will facilitate the query ability which help to enhance the analyzing process. In additon, an ELT pipeline is build by using Python to collect data fom the repo directories then stored in the designed tables, database. 

### Database design
Using the song and log datasets, I create a star schema optimized for queries on song play analysis. This includes one fact table (songplay) and 4 dimension tables (users, songs, artists and time). Folowing is the information about the sparkify database schema and tables. 
1. The fact table - songplay contains all the information about records in log data associated with song plays. This table has 9 columns songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent.
2. Dimension users table contains information about users in the app like user_id, first_name, last_name, gender, level
3. Dimension songs table has information of all songs in music database like song_id, title, artist_id, year, duration
4. Dimension artists table has information of all artists in music database, including artist_id, name, location, latitude, longitude
5. Dimension time table contains timestamps of records in songplays broken down into specific units: start_time, hour, day, week, month, year, weekday

<img src="./sparkifydb_erd.png" width="80%"/>

### ETL Process
Sparkify data consist song dataset and log dataset. Song dataset is stored in 'data/song_data' directory which consist of multiple files in JSON format contain metadata about song and artist of that song. The files are partitioned by the first three letters of each song's track ID. Data fom this dataset was extracted and inserted to dimention songs and artists tables.
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.The log files in the dataset are partitioned by year and month and is stored in 'data/log_data' directory. Data from this dataset is extracted and inserted to time and users tables dimendion. Since the log file does not specify an ID for either the song or the artist, in ordet to get the song ID and artist ID by querying the songs and artists tables to find matches based on song title, artist name, and song duration time.

### Project Repository files
There are 5 files and a data repo directory in this project. The sql_queries.py contains all the SQL queries to drop, create, insert data to tables as well as select data. The create_tables file will be used to connect to postgres database then drop, create Sparkify database and then implemente the SQL queries that is imported from the sql_queries.py file. The etl.ipynb is used to build and test the ETL pipeline for single test file data. The etl.py is build to run for the entire dataset of Sparkify. In addition, file test.ipynb is used to find are there any potential mistakes present during the entire process like, create tables and their parameter constraints and ETL process.

### How To Run the Project
To run the ETL pipeline, open new Console then import create_tables, etl modules. Then run create_tables module (create_tables.main()) to create database, tables, once run successfull we will run etl module (etl.main()). test.ipynb is used to test whether designed database and tables available and contain the required data or not.

### Example analytics query to find the playsong
%sql SELECT * FROM songplays WHERE song_id is not null and artist_id is not null LIMIT 5;
